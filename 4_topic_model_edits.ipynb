{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the revisions as a datafram\n",
    "file_index = [1, 2, 3, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
    "\n",
    "edits_df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(file_index)):\n",
    "    import_file = 'revisions' + str(file_index[i]) + '_read4edits.csv'\n",
    "    edits_df = pd.concat([edits_df, pd.read_csv(import_file)], axis=0)\n",
    "    \n",
    "edits_df.reset_index(inplace = True, drop = True)\n",
    "edits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert diff_text from a string to a list\n",
    "from tqdm import tqdm\n",
    "\n",
    "diff_text_string_list = list(edits_df.diff_text)\n",
    "diff_text_list_list = []\n",
    "\n",
    "for item in tqdm(diff_text_string_list):\n",
    "    diff_text_list_list.append(list(filter((', ').__ne__, item.split(\"'\")[1:-1])))\n",
    "    \n",
    "edits_df.drop(['diff_text'], axis = 1, inplace = True)\n",
    "\n",
    "edits_df['diff_text'] = diff_text_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop documents with less than a specified number of words\n",
    "threshold = 50\n",
    "\n",
    "lengths = []\n",
    "for i in edits_df.diff_text:\n",
    "    lengths.append(len(i))\n",
    "\n",
    "for i in tqdm(range(len(lengths))):\n",
    "    if lengths[i] < threshold:\n",
    "        edits_df = edits_df.drop([i])\n",
    "\n",
    "edits_df.reset_index(inplace = True, drop = True)\n",
    "        \n",
    "print(\"Number of documents remaining: %d\" % edits_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary\n",
    "from collections import Counter\n",
    "\n",
    "docs_complete = list(edits_df.diff_text)\n",
    "\n",
    "vocab = Counter()\n",
    "for doc in tqdm(docs_complete):\n",
    "    vocab.update(doc)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_import = stopwords.words('english')\n",
    "\n",
    "stopwords_additional = ['ref', 'url', 'n', 'br', 'user', 'talk', 'color', 'style', \\\n",
    "                       'wikipedia', 'web', 'date', 'utc', 'align', 'name', 'cite', \\\n",
    "                       'date', 'c', 'title', 'archive', 'flagicon', 'links', 'order' \\\n",
    "                       'center', 'sort', 'label', 'cvt', 'abbr', 'symbol', 'publisher', \\\n",
    "                       'category', 'convert', 'style', 'width', 'accessdate', 'nbsp', \\\n",
    "                       'language', 'km', 'row', 'nthe', 'access', 'website', 'x', 'infobox', \\\n",
    "                       'wikiproject', 'image', 'nimage', 'short', 'description', 'class', \\\n",
    "                       'character', 't', 'ts', 'u', 'ns', 'g', 'lat', 'fb', 'bul', 'gk', \\\n",
    "                       'update', 'j', 'p', 'fs', 'q', 'link', 'file', 'svg', 'list', \\\n",
    "                       'fig', 'pog', 'df', 'altname', 'piccap', 'use', 'mdy', 'expand', \\\n",
    "                       'date', 'first', 'last', 'work', 'fact', 'check', 'background', \\\n",
    "                       'language', 'aus', 'rus', 'chn', 'cze', 'fra', 'ger', 'ita', \\\n",
    "                       'de', 'also', 'one', 'bgcolor', 'year', 'two', 'time', 'would', \\\n",
    "                       'new', 'many', 'text', 'sup', 'pos', 'nat', 'req', 'sent', 'go', \\\n",
    "                       'f', 'rowspan', 'jpg', 'w', 'r', 'ndash', 'cfcfff', 'dfffdf', \\\n",
    "                       'hex', 'efcfff', 'none', 'und', 'ii', 'including', 'since', 'non', \\\n",
    "                       'valign', 'id', 'colspan', 'font', 'mf', 'au', 'used', 'wpships', \\\n",
    "                       'wpmilhist', 'infobox', 'dcecfc', 'like', 'we', 'your', 'ii', 'did', \\\n",
    "                       'should', 'very', 'td', 'those', 'another', 'does', 'di', 'el', \\\n",
    "                        'del', 'la', 'sortname', 'sublist', 'could','make', 'wp', 'please', \\\n",
    "                        'hi', 'wikitable', 'efefef', 'flagathlete', 'often', 'became', \\\n",
    "                        'called', 'hs', 'fi', 'te', 'pp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim words that occur too frequently or too rarely\n",
    "vocab = Counter(token for token in tqdm(vocab.elements()) if vocab[token] > 50)\n",
    "vocab = Counter(token for token in tqdm(vocab.elements()) if token not in stopwords_import)\n",
    "vocab = Counter(token for token in tqdm(vocab.elements()) if token not in stopwords_additional)\n",
    "\n",
    "# Update the documents\n",
    "docs_filtered = [[token for token in doc if token in vocab] for doc in docs_complete]\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs_filtered)\n",
    "_ = dictionary[0]  # This sort of \"initializes\" dictionary.id2token\n",
    "\n",
    "# Vectorize data / Bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create author2doc dictionaries\n",
    "author2doc_country = dict()\n",
    "author2doc_org = dict()\n",
    "\n",
    "for index, row in tqdm(edits_df.iterrows()):\n",
    "    \n",
    "    country = row['country']\n",
    "    org = row['org']\n",
    "    \n",
    "    # This is a new author\n",
    "    if not author2doc_country.get(country):\n",
    "        author2doc_country[country] = []\n",
    "    if not author2doc_org.get(org):\n",
    "        author2doc_org[org] = []\n",
    "    \n",
    "    # Add document IDs to author\n",
    "    author2doc_country[country].extend([index])\n",
    "    author2doc_org[org].extend([index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary, author2doc dictionaries, and updated documents\n",
    "dictionary.save('dictionary_edits')\n",
    "\n",
    "np.save('author2doc_edits.npy', author2doc_country) \n",
    "\n",
    "import pickle\n",
    "pickle.dump(docs_filtered, open(\"docs_filtered_edits.txt\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set number of passes and iterations to ensure convergence (performed on small sample only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import logging\n",
    "import os\n",
    "from gensim.models import AuthorTopicModel\n",
    "\n",
    "author2doc = author2doc_country\n",
    "\n",
    "topic_num = 10\n",
    "passes = 50\n",
    "iterations = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(filename='gensim_edits.log',\n",
    "                    format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                    level=logging.INFO)\n",
    "\n",
    "for iteration in iterations:\n",
    "    %time model = AuthorTopicModel(corpus=corpus, id2word=dictionary.id2token, author2doc=author2doc, \\\n",
    "                                   num_topics=topic_num, \\\n",
    "                                   chunksize=5000, passes=passes, eval_every=1, iterations=iteration, \\\n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the likelihood to check for convergence\n",
    "\n",
    "p = re.compile(\"(-*\\d+\\.\\d+) per-word .* (\\d+\\.\\d+) perplexity\")\n",
    "matches = [p.findall(l) for l in open('gensim_edits.log')]\n",
    "matches = [m for m in matches if len(m) > 0]\n",
    "tuples = [t[0] for t in matches]\n",
    "perplexity = [float(t[1]) for t in tuples]\n",
    "likelihood = [float(t[0]) for t in tuples]\n",
    "iter = list(range(0,passes))\n",
    "\n",
    "plt.plot(iter,np.array(likelihood).reshape(len(iterations), int(len(likelihood)/len(iterations))).T)\n",
    "plt.ylabel(\"Log Likelihood\")\n",
    "plt.xlabel(\"Pass\")\n",
    "plt.title(\"Topic Model Convergence\")\n",
    "plt.legend(iterations);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with optimal topic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "\n",
    "author2doc = author2doc_country\n",
    "\n",
    "num_topics = [2, 4, 6, 8, 10, 15, 20, 25, 30]\n",
    "passes = 10\n",
    "iterations = 15\n",
    "\n",
    "model_list = []\n",
    "\n",
    "for topic_num in num_topics:\n",
    "    %time model = AuthorTopicModel(corpus=corpus, \\\n",
    "                                   id2word=dictionary.id2token, \\\n",
    "                                   author2doc=author2doc, \\\n",
    "                                   num_topics=topic_num, \\\n",
    "                                   chunksize=5000, \\\n",
    "                                   passes=passes, \\\n",
    "                                   eval_every=1, \\\n",
    "                                   iterations=iterations, \\\n",
    "                                   random_state=0)\n",
    "    \n",
    "    model_coherence = CoherenceModel(model = model, texts = docs_filtered, dictionary = dictionary, coherence = 'c_v')\n",
    "    model_list.append((model, model_coherence.get_coherence()))\n",
    "\n",
    "model_selected = max(model_list, key=lambda x: x[1])[0]\n",
    "model_selected.save('edits_model2')\n",
    "topic_num_selected = num_topics[np.argmax([x[1] for x in model_list])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the topic coherence\n",
    "import matplotlib.pyplot as plt\n",
    "coherence = [float(t[1]) for t in model_list]\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.plot(num_topics, coherence)\n",
    "plt.ylabel(\"Topic Coherence\")\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.title(\"Optimal Topic Selection\")\n",
    "plt.savefig('topic_coherence_edits.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top words in each topic\n",
    "num_words = 50\n",
    "\n",
    "top_words = pd.DataFrame({'word rank': np.arange(1,num_words+1)})\n",
    "for k in np.arange(topic_num_selected): \n",
    "    topic = model_selected.get_topic_terms(k, num_words)\n",
    "    words = [dictionary.id2token[topic[i][0]] for i in np.arange(num_words)]\n",
    "    probs = [topic[i][1] for i in np.arange(num_words)]\n",
    "    top_words['topic %d' % k] = words\n",
    "\n",
    "top_words.set_index('word rank', inplace = True)\n",
    "\n",
    "# Display the results\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of authors over topics\n",
    "import seaborn as sns\n",
    "\n",
    "author_topic_df = pd.DataFrame(columns=list(top_words.columns), index = list(author2doc.keys()))\n",
    "\n",
    "for author in list(author2doc.keys()):\n",
    "    temp_df = pd.DataFrame(model_selected[author], columns = ['topic_num', 'prob'])\n",
    "    for i in range(topic_num_selected):\n",
    "        try: \n",
    "            author_topic_df[author_topic_df.columns[i]][author] = temp_df.loc[i, 'prob']\n",
    "        except:\n",
    "            author_topic_df[author_topic_df.columns[i]][author] = 0\n",
    "\n",
    "author_topic_df = author_topic_df.astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (10, 7.5))\n",
    "sns.heatmap(author_topic_df, cmap = 'Blues')\n",
    "plt.savefig('heatmap_edits.pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot interactive tsne\n",
    "%time\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "smallest_author = 0  # Ignore authors with documents less than this.\n",
    "authors = [model_selected.author2id[a] for a in model_selected.author2id.keys() if len(model_selected.author2doc[a]) >= smallest_author]\n",
    "_ = tsne.fit_transform(model_selected.state.gamma[authors, :])  # Result stored in tsne.embedding_\n",
    "\n",
    "# Tell Bokeh to display plots inside the notebook.\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "\n",
    "x = tsne.embedding_[:, 0]\n",
    "y = tsne.embedding_[:, 1]\n",
    "author_names = [model_selected.id2author[a] for a in authors]\n",
    "\n",
    "# Radius of each point corresponds to the number of documents attributed to that author.\n",
    "scale = 0.01\n",
    "author_sizes = [len(model_selected.author2doc[a]) for a in author_names]\n",
    "radii = [size * scale for size in author_sizes]\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            author_names=author_names,\n",
    "            author_sizes=author_sizes,\n",
    "            radii=radii,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add author names and sizes to mouse-over info.\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "        (\"author\", \"@author_names\"),\n",
    "        (\"size\", \"@author_sizes\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p = figure(tools=[hover, 'crosshair,pan,wheel_zoom,box_zoom,reset,save,lasso_select'])\n",
    "p.scatter('x', 'y', radius='radii', source=source, fill_alpha=0.6, line_color=None)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot static tsne\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x, y, s=[x * 250 for x in radii], alpha=0.6)\n",
    "for i in range(len(x)):\n",
    "    plt.annotate(author_names[i], (x[i], y[i]), ha='center')\n",
    "plt.savefig('tsne_static_edits.pdf');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
