{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update filename for each new file from revisions1 to revisions27\n",
    "filename = 'revisions1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download packages\n",
    "import pandas as pd\n",
    "\n",
    "# Load the revisions as a dataframe\n",
    "revisions = pd.read_csv(filename + '.csv')\n",
    "revisions_df = pd.DataFrame(revisions)\n",
    "\n",
    "# Rename columns\n",
    "revisions_df = revisions_df.rename(columns={\"id\": \"page_id\", \"title\": \"page_title\"})\n",
    "\n",
    "# Convert to ID to integer for pywikibot\n",
    "revisions_df['rev_id'] = revisions_df['rev_id'].astype(int)\n",
    "\n",
    "revisions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IP address list\n",
    "import netaddr\n",
    "import csv\n",
    "\n",
    "# Load the IP address ranges as a dataframe\n",
    "ip_database = open('ip_list.csv')\n",
    "ip_reader = csv.reader(ip_database)\n",
    "ip_df = pd.DataFrame(ip_reader)\n",
    "\n",
    "# Set the column names\n",
    "ip_df.columns = ip_df.iloc[0]\n",
    "ip_df = ip_df.drop(ip_df.index[0])\n",
    "\n",
    "# Define a function to split the IP addresses (stored as strings) into a list\n",
    "def split(s):\n",
    "    parts = []\n",
    "    bracket_level = 0\n",
    "    current = []\n",
    "    # Remove special-case of trailing characters\n",
    "    for c in (s + \",\"):\n",
    "        if c == \",\" and bracket_level == 0:\n",
    "            parts.append(\"\".join(current))\n",
    "            current = []\n",
    "        else:\n",
    "            if c == \"[\":\n",
    "                bracket_level += 1\n",
    "            elif c == \"]\":\n",
    "                bracket_level -= 1\n",
    "            current.append(c)\n",
    "    return parts\n",
    "\n",
    "# Create a list of IP_ranges\n",
    "ip_list_bunched = []\n",
    "\n",
    "for i in list(ip_df['ip_range']):\n",
    "    ip_list_bunched.append(split(i[1:-1]))\n",
    "\n",
    "# Convert CIDR to IP ranges\n",
    "for i in range(len(ip_list_bunched)):\n",
    "    for j in range(len(ip_list_bunched[i])):\n",
    "        if '[' in ip_list_bunched[i][j]:\n",
    "            # Remove whitespace and leading and trailing inverted commas\n",
    "            startip = ip_list_bunched[i][j].split()[0].strip('[], ')[1:-1]\n",
    "            endip = ip_list_bunched[i][j].split()[1].strip('[], ')[1:-1]\n",
    "\n",
    "            # Convert to CIDR as a string\n",
    "            temp_ip = netaddr.iprange_to_cidrs(startip, endip)[0]\n",
    "\n",
    "            # Replace value in list\n",
    "            ip_list_bunched[i][j] = temp_ip\n",
    "        else:\n",
    "            # Strip the leading and trailing inverted commas\n",
    "            temp_ip = ip_list_bunched[i][j].strip('\\'\\\" ')\n",
    "            ip_list_bunched[i][j] = netaddr.IPNetwork(temp_ip)\n",
    "\n",
    "# Convert each sub-list of ipnetworks to a set\n",
    "ip_set_bunched = []\n",
    "\n",
    "for i in range(len(ip_list_bunched)):\n",
    "    ip_set_bunched.append(netaddr.IPSet(ip_list_bunched[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load missing title and pageids; load previous revision id number\n",
    "import requests\n",
    "import pywikibot\n",
    "from tqdm import tqdm\n",
    "import concurrent\n",
    "import numpy as np\n",
    "\n",
    "# Set up requests\n",
    "S = requests.Session()\n",
    "S.mount('http://en.wikipedia.org/w/api.php', requests.adapters.HTTPAdapter(max_retries = 10))\n",
    "URL = \"http://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# Set up pywikibot\n",
    "site = pywikibot.Site(\"en\", \"wikipedia\")\n",
    "\n",
    "bar = tqdm()\n",
    "\n",
    "def run_iter(index, row):\n",
    "    if index < 0:\n",
    "        return\n",
    "    \n",
    "    # Replace page id if missing\n",
    "    if row['page_id'] == -1:\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"revisions\",\n",
    "            \"titles\": row['page_title'],\n",
    "            \"rvlimit\": \"1\",\n",
    "            \"rvdir\": \"newer\",\n",
    "            \"rvstartid\": str(row['rev_id']),\n",
    "            \"rvdiffto\" : \"prev\",\n",
    "            \"formatversion\": \"2\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "        checked_id = DATA['query']['pages'][0]['pageid']\n",
    "        revisions_df.loc[index, 'page_id'] = checked_id\n",
    "        \n",
    "    # Find previous revision id\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"pageids\": revisions_df.loc[index, 'page_id'],\n",
    "        \"rvlimit\": \"1\",\n",
    "        \"rvdir\": \"newer\",\n",
    "        \"rvstartid\": str(row['rev_id']),\n",
    "        \"rvdiffto\" : \"prev\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "    \n",
    "    try:\n",
    "        rev_id_prev = DATA['query']['pages'][0]['revisions'][0]['diff']['from']\n",
    "        revisions_df.loc[index, 'rev_id_prev'] = int(rev_id_prev)\n",
    "    \n",
    "    except:\n",
    "        rev_id_prev = 0\n",
    "    \n",
    "    # Replace title if missing\n",
    "    if not isinstance(revisions_df.loc[index, 'page_title'], str):\n",
    "        checked_title = DATA['query']['pages'][0]['title']\n",
    "        revisions_df.loc[index, 'page_title'] = checked_title\n",
    "            \n",
    "    # Import document text before and after revision\n",
    "    try: \n",
    "        page = pywikibot.Page(site, revisions_df.loc[index, 'page_title'])\n",
    "        revisions_df.loc[index, 'new_text'] = page.getOldVersion(oldid = row['rev_id'])\n",
    "    except: # To accommodate pages whose title has changed\n",
    "        page = pywikibot.Page(site, DATA['query']['pages'][0]['title'])\n",
    "        revisions_df.loc[index, 'new_text'] = page.getOldVersion(oldid = row['rev_id'])\n",
    "\n",
    "    if rev_id_prev == 0:\n",
    "        revisions_df.loc[index, 'old_text'] = ''\n",
    "    else:\n",
    "        revisions_df.loc[index, 'old_text'] = page.getOldVersion(oldid = rev_id_prev)\n",
    "        \n",
    "    # Import country names\n",
    "    for i in range(len(ip_set_bunched)):\n",
    "        if netaddr.IPNetwork(str(row['ip'])) in ip_set_bunched[i]:\n",
    "            country = ip_df.iloc[i, 0]\n",
    "            org = ip_df.iloc[i, 1]\n",
    "            revisions_df.loc[index, 'country'] = country\n",
    "            revisions_df.loc[index, 'org'] = org\n",
    "            break\n",
    "        \n",
    "    bar.update(1)\n",
    "\n",
    "num_concurrent = 20\n",
    "pool = concurrent.futures.ThreadPoolExecutor(max_workers=num_concurrent)\n",
    "futures = []\n",
    "for k, g in revisions_df.groupby(np.arange(len(revisions_df))//num_concurrent):\n",
    "    for index, row in g.iterrows():\n",
    "        f = pool.submit(run_iter, index, row)\n",
    "        futures.append(f)\n",
    "    concurrent.futures.wait(futures)\n",
    "    futures = []\n",
    "bar.close()\n",
    "pool.shutdown()\n",
    "            \n",
    "revisions_df.to_csv(filename + '_temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean missing rows\n",
    "missing_old = revisions_df[revisions_df['old_text'].isna()]\n",
    "revisions_2 = revisions_df[revisions_df['old_text'].notna()]\n",
    "\n",
    "missing_new = revisions_2[revisions_2['new_text'].isna()]\n",
    "revisions_3 = revisions_2[revisions_2['new_text'].notna()]\n",
    "\n",
    "missing_text = pd.concat([missing_old, missing_new], axis=0)\n",
    "missing_text.to_csv(filename + '_missing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing country and org names\n",
    "missing_country = revisions_3[revisions_3['country'].isna()]\n",
    "revisions_4 = revisions_3[revisions_3['country'].notna()]\n",
    "\n",
    "missing_org = revisions_4[revisions_4['org'].isna()]\n",
    "revisions_5 = revisions_4[revisions_4['org'].notna()]\n",
    "\n",
    "missing_author = pd.concat([missing_country, missing_org], axis=0)\n",
    "\n",
    "# Add country and org names back to missing rows\n",
    "from tqdm import tqdm \n",
    "\n",
    "for index, row in tqdm(missing_author.iterrows()):\n",
    "    for i in range(len(ip_set_bunched)):\n",
    "        if netaddr.IPNetwork(str(row['ip'])) in ip_set_bunched[i]:\n",
    "            country = ip_df.iloc[i, 0]\n",
    "            org = ip_df.iloc[i, 1]\n",
    "            missing_author.loc[index, 'country'] = country\n",
    "            missing_author.loc[index, 'org'] = org\n",
    "            break\n",
    "            \n",
    "revisions_6 = pd.concat([revisions_5, missing_author], axis=0)\n",
    "revisions_6.to_csv(filename + '_complete.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
